
<html>
<head>
  <meta charset="utf-8">
  <title>Gender Artifacts in Visual Datasets</title>

  <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet'>
  <link rel="stylesheet" href="font-awesome-4.7.0/css/font-awesome.min.css">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
  <link href="mainpage.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>


<!-- Lab logo -->
<!--<div class="row" style="text-align:center;padding:0;margin:0;padding-top:40;padding-bottom:10">-->
<div class="row" style="text-align:center;padding:0;padding-top:20;padding-bottom:10;margin:0">
  <div class="container">
    <img src="imgs/princetonlogo.png" height="40px" style="vertical-align:middle">
    <span style="font-size:32px;vertical-align:middle"><a href="https://visualai.princeton.edu" style="color:#ff8f00" target="_blank">Princeton Visual AI Lab</a></span>
  </div>
</div>

<!-- Authors -->
<div class="container-fluid">
  <div class="row">
    <h1><span style="font-size:36px;color:#333;font-weight:800">Gender Artifacts in Visual Datasets</span></h1>
    <div class="authors">
      <span style="font-size:18px"><a href="https://nicolemeister.github.io/" style="color:#1075bc" target="new">Nicole Meister*</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://dorazhao99.github.io/" style="color:#1075bc" target="new">Dora Zhao*</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://angelina-wang.github.io/" style="color:#1075bc" target="new">Angelina Wang</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://www.cs.princeton.edu/~vr23/" style="color:#1075bc" target="new">Vikram V. Ramaswamy</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://www.ruthfong.com/" style="color:#1075bc" target="new">Ruth Fong</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://www.cs.princeton.edu/~olgarus/" style="color:#1075bc" target="new">Olga Russakovsky</a></span>
      <br>
      <span style="font-size:18px">Princeton University</span>
      <br>
      <span style="font-size:18px">{nmeister, dorothyz}@alumni.princeton.edu<br><br></span>
    </div>
  </div>
</div>

<!-- Figure 1 -->
<!-- <div class="row" style="text-align:center;padding:0;margin:0">
  <div class="container">
    <figure>
     <img src="imgs/PullFigure_large.png" style="width:70%">
      <figcaption>
        Training a visual classifier for an attribute (e.g., wearing hat) can be complicated by correlations in the training data.
        For example, the presence of hats can be correlated with the presence of glasses.
        We propose a dataset augmentation strategy using Generative Adversarial Networks (GANs)
        that successfully removes this correlation by adding or removing glasses from existing images, creating a balanced dataset.
      </figcaption>
    </figure>

  </div>
</div> -->

<!-- Icons -->
<div class="container-fluid">
  <div class="row">
    <!-- <div class="col-lg-0 col-md-0 col-sm-0"></div> -->

    <!--<div class="col-xs-2 col-xs-offset-1 text-center">-->
    <div class="col-xs-2 col-xs-offset-3 text-center">
      <div class="service-box mt-5 mx-auto">
        <h4 class="mb-3" style="font-size:18px">
          <a href="https://arxiv.org/abs/2206.09191" target="_blank"> 
            <i class="fa fa-4x fa-file-text-o text-primary mb-3"></i>
          </a>
        </h4>
        Paper
      </div>
    </div>


    <div class="col-xs-2 col-xs-offset-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <h4 class="mb-3" style="font-size:18px">
          <a href="https://github.com/dorazhao99/gender-artifacts" target="_blank"> 
            <i class="fa fa-4x fa-github text-primary mb-3"></i>
          </a>
        </h4>
        Code
      </div>
    </div>


    <!-- <div class="col-xs-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <h4 class="mb-3" style="font-size:18px">
          <a href="https://forms.gle/FBi3ZsMDficweyP96" target="_blank" class="button">Annotations </a>
        </h4>
      </div>
    </div> 
--> 


    <!--
    <div class="col-xs-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://youtu.be/mVU5tSxS4is" target="_blank">
          <i class="fa fa-2x fa-video-camera text-primary mb-3 "></i>
        </a>
        <h4 class="mb-3" style="font-size:18px">Talk</h4>
      </div>
    </div>
    -->

    <!--
    <div class="col-xs-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://github.com/sunniesuhyoung/DST/blob/master/dst.bib" target="_blank">
          <i class="fa fa-3x fa-quote-right text-primary mb-3 "></i>
        </a>
        <h4 class="mb-3" style="font-size:18px">Bibtex</h4>
      </div>
    </div>
    -->

  </div>
</div>

<!-- Abstract -->
<div class="container">
  <h2>Abstract</h2>
  Gender biases are known to exist within large-scale visual datasets and can be reflected or even amplified in downstream models. 
  Many prior works have proposed methods for mitigating gender biases, often by attempting to remove gender expression information from images. 
  To understand the feasibility and practicality of these approaches, we investigate what <em>gender artifacts </em> exist within large-scale visual datasets.
  We define a <em>gender artifact </em> as a visual cue that is correlated with gender, focusing specifically on those cues that are learnable by a modern image classifier and have an interpretable human corollary. 
  Through our analyses, we find that gender artifacts are ubiquitous in the COCO and OpenImages datasets, 
  occurring everywhere from low-level information (e.g., the mean value of the color channels) to the higher-level composition of the image 
  (e.g., pose and location of people). Given the prevalence of gender artifacts, 
  we claim that attempts to remove gender artifacts from such datasets are largely infeasible. 
  Instead, the responsibility lies with researchers and practitioners to be aware that the distribution of images within datasets is 
  highly gendered and hence develop methods which are robust to these distributional shifts across groups.

</div>

<!-- Citation -->
<div class="container">
  <h2>Citation</h2>
  <pre><code>
    @article{meister2022artifacts,
    author = {Nicole Meister and Dora Zhao and Angelina Wang and Vikram V. Ramaswamy and Ruth Fong and Olga Russakovsky},
    title = {Gender Artifacts in Visual Datasetsi},
    journal = {CoRR},
    volume = {abs/2206.09191},
    year={2022}
    }
  </code></pre>
</div>

<!-- Talk -->
<!--
<div class="container" >
  <h2>1 Minute Summary</h2>
  <div>
    <div style="width: 75%;height: 0;padding-bottom: 42%;position: relative;margin-left: auto;margin-right: auto;">
      <iframe src="https://youtube.com/embed/7qUzfcn6TPk" allowfullscreen style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"></iframe>
    </div>
  </div>
</div>
-->


<!-- Method -->
<div class="container">
<h2>Identifying Gender Artifacts</h2>

In our work, we explore to what extent gendered information can truly be removed from the dataset. 
To do so, we develop a framework to identify gender artifacts, or visual cues that are correlated with gender. 
While there can be infinitely many potential correlations in an image, we focus specifically on those that are learnable 
(i.e., result in a learnable difference for a machine learning model) and interpretable 
(i.e., have an interpretable human corollary such as color and shape). 
To discover gender artifacts, we take the approach of leveraging a "gender artifact model," 
which is a model trained on selectively manipulated versions of the dataset (e.g., grayscale, occluded background) 
to predict perceived gender expression. 
This method of discovering gender artifacts is more complex than prior work, 
as it goes beyond analyzing annotated attributes in the image to unveil a broader set of gender artifacts. 

<br><br>
Our goal is to understand what the model is picking up on as a predictive feature, 
and how variations of the dataset with particular gender artifacts removed or added may increase or decrease its performance. 
<br><br>
<center>
  <img src="imgs/3_25 Bias in AI Presentation (2).jpg" style="width:60%">
</center>

We analyze some of the higher-level perceptual components of the image by changing the resolution and color of the image (1).
Then, we turn to disentangling gender artifacts arising from the person versus the background (2). 
Finally, we investigate try to distinguish between two components of the background: the objects and the “scene” by studying the role of contextual objects as gender artifacts (3).

<br><br>
It is important to note that we do not condone the use of automated gender recognition in practice and discuss additional important ethical considerations in our paper's Introduction (Sec 1) and Set-up (Sec 3.3). 


</div>

<!-- Analysis -->
<div class="container">
<h2>Implications</h2>
  
<ul>
  <li>Many prior works have proposed methods for mitigating gender biases by removing gender expression information from the image. 
In contrast, we show that gender artifacts are intricately embedded in visual datasets, 
such that attempting to remove them in computer vision bias mitigation techniques may be a futile endeavor.
  his is evidenced by experiments where even after the person is <em>entirely occluded </em> with a rectangular mask, 
  the gender artifact model is nevertheless able to reliably distinguish which of the two genders is present in the image:
  AUC of 70.8% in COCO and 63.0% in OpenImages. 
  We more realistically advise practitioners to adopt <em>fairness-aware</em> models and disaggregated evaluation metrics, 
  where practitioners explicitly account for potential discrepancies between protected demographic groups. 
  </li>
  <li>Since there are so many salient artifacts in image datasets that are (spuriously) correlated with gender, 
  our findings point to an incoherence of gender prediction. 
  Any time a model predicts "gender", 
  we should wonder what the prediction refers to, as it could just as easily be the gender of the person most likely to be in a redder 
  image as opposed to a greener image, rather than a form of societally meaningful gender expression. </li>
</ul>  


</div>

<!-- Related Work -->
<div class="container" >
  <!-- <h2>Related Work</h2>
  <div>
  Below are some papers related to our work. We discuss them in more detail in the related work section of our paper.
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/1906.06439" target="_blank">Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias.</a>
    Emily Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, Andrew Zaldivar.
    CVPR 2019 Workshop on Fairness Accountability Transparency and Ethics in Computer Vision.
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/1805.09910" target="_blank"> Fairness GAN.</a>
    Prasanna Sattigeri, Samuel C. Hoffman, Vijil Chenthamarakshan, Kush R. Varshney.
    IBM Journal of Research and Development 2019.
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/2004.06524" target="_blank">Contrastive Examples for Addressing the Tyranny of the Majority.</a>
    Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, Novi Quadrianto. arXiv 2020
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/1910.12008" target="_blank">Fair Generative Modeling via Weak Supervision.</a>
    Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, Stefano Ermon. ICML 2020
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/2007.06570" target="_blank">Towards Causal Benchmarking of Bias in Face Analysis Algorithms.</a>
    Guha Balakrishnan, Yuanjun Xiong, Wei Xia, Pietro Perona. ECCV 2020
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/1911.11834" target="_blank">Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation.</a>
    Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, Olga Russakovsky. CVPR 2020
  </div> -->
</div>

<!-- Acknowledgements -->
<div class="container">
  <h2>Acknowledgements</h2>

  This material is based upon work supported by the National Science Foundation under Grant No. 1763642, Grant No. 2112562, 
  and the Graduate Research Fellowship to A.W., as well as by the Princeton Engineering Project X Fund. 
  Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) 
  and do not necessarily reflect the views of the National Science Foundation.
  <br/><br/>

  The webpage template was adapted from this
  <a href="https://princetonvisualai.github.io/gan-debiasing/" target="_blank">project page</a>.

</div>

<div class="container" >
  <h2>Contact</h2>
  <div><a href="https://nicolemeister.github.io/" style="color:#1075bc" target="new">Nicole Meister</a> (nmeister@alumni.princeton.edu)</div>
  <div><a href="https://dorazhao99.github.io/" style="color:#1075bc" target="new">Dora Zhao</a> (dorothyz@alumni.princeton.edu)</div>
</div>

<div id="footer">
</div>


</body>
</html>
