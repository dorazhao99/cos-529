
<html>
<head>
  <meta charset="utf-8">
  <title>Gender Artifacts in Visual Datasets</title>

  <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet'>
  <link rel="stylesheet" href="font-awesome-4.7.0/css/font-awesome.min.css">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
  <link href="mainpage.css" rel="stylesheet">
</head>

<body>


<!-- Lab logo -->
<!--<div class="row" style="text-align:center;padding:0;margin:0;padding-top:40;padding-bottom:10">-->
<div class="row" style="text-align:center;padding:0;padding-top:20;padding-bottom:10;margin:0">
  <div class="container">
    <img src="imgs/princetonlogo.png" height="40px" style="vertical-align:middle">
    <span style="font-size:32px;vertical-align:middle"><a href="https://visualai.princeton.edu" style="color:#ff8f00" target="_blank">Princeton Visual AI Lab</a></span>
  </div>
</div>

<!-- Authors -->
<div class="container-fluid">
  <div class="row">
    <h1><span style="font-size:36px;color:#333;font-weight:800">Gender Artifacts in Visual Datasets</span></h1>
    <div class="authors">
      <span style="font-size:18px"><a href="https://nicolemeister.github.io/" style="color:#1075bc" target="new">Nicole Meister*</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://dorazhao99.github.io/" style="color:#1075bc" target="new">Dora Zhao*</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://angelina-wang.github.io/" style="color:#1075bc" target="new">Angelina Wang</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://www.cs.princeton.edu/~vr23/" style="color:#1075bc" target="new">Vikram V. Ramaswamy</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://www.ruthfong.com/" style="color:#1075bc" target="new">Ruth Fong</a></span>
      &nbsp;
      <span style="font-size:18px"><a href="https://www.cs.princeton.edu/~olgarus/" style="color:#1075bc" target="new">Olga Russakovsky</a></span>
      <br>
      <span style="font-size:18px">Princeton University</span>
      <br>
      <span style="font-size:18px">{nmeister, dorothyz}@alumni.princeton.edu<br><br></span>
    </div>
  </div>
</div>

<!-- Figure 1 -->
<!-- <div class="row" style="text-align:center;padding:0;margin:0">
  <div class="container">
    <figure>
     <img src="imgs/PullFigure_large.png" style="width:70%">
      <figcaption>
        Training a visual classifier for an attribute (e.g., wearing hat) can be complicated by correlations in the training data.
        For example, the presence of hats can be correlated with the presence of glasses.
        We propose a dataset augmentation strategy using Generative Adversarial Networks (GANs)
        that successfully removes this correlation by adding or removing glasses from existing images, creating a balanced dataset.
      </figcaption>
    </figure>

  </div>
</div> -->

<!-- Icons -->
<div class="container-fluid">
  <div class="row">
    <!-- <div class="col-lg-0 col-md-0 col-sm-0"></div> -->

    <!--<div class="col-xs-2 col-xs-offset-1 text-center">-->
    <div class="col-xs-2 col-xs-offset-3 text-center">
      <div class="service-box mt-5 mx-auto">
        <h4 class="mb-3" style="font-size:18px">
          <a href="https://arxiv.org/abs/2206.09191" target="_blank" class="button">Paper </a>
        </h4>
      </div>
    </div>

    <div class="col-xs-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <h4 class="mb-3" style="font-size:18px">
          <a href="https://github.com/dorazhao99/gender-artifacts" target="_blank" class="button">Code </a>
        </h4>
      </div>
    </div>


    <!-- <div class="col-xs-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <h4 class="mb-3" style="font-size:18px">
          <a href="https://forms.gle/FBi3ZsMDficweyP96" target="_blank" class="button">Annotations </a>
        </h4>
      </div>
    </div> 
--> 


    <!--
    <div class="col-xs-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://youtu.be/mVU5tSxS4is" target="_blank">
          <i class="fa fa-2x fa-video-camera text-primary mb-3 "></i>
        </a>
        <h4 class="mb-3" style="font-size:18px">Talk</h4>
      </div>
    </div>
    -->

    <!--
    <div class="col-xs-2 text-center">
      <div class="service-box mt-5 mx-auto">
        <a href="https://github.com/sunniesuhyoung/DST/blob/master/dst.bib" target="_blank">
          <i class="fa fa-3x fa-quote-right text-primary mb-3 "></i>
        </a>
        <h4 class="mb-3" style="font-size:18px">Bibtex</h4>
      </div>
    </div>
    -->

  </div>
</div>

<!-- Abstract -->
<div class="container">
  <h2>Abstract</h2>
  Gender biases are known to exist within large-scale visual datasets and can be reflected or even amplified in downstream models. 
  Many prior works have proposed methods for mitigating gender biases, often by attempting to remove gender expression information from images. 
  To understand the feasibility and practicality of these approaches, we investigate what <em>gender artifacts </em> exist within large-scale visual datasets.
  We define a <em>gender artifact </em> as a visual cue that is correlated with gender, focusing specifically on those cues that are learnable by a modern image classifier and have an interpretable human corollary. 
  Through our analyses, we find that gender artifacts are ubiquitous in the COCO and OpenImages datasets, 
  occurring everywhere from low-level information (e.g., the mean value of the color channels) to the higher-level composition of the image 
  (e.g., pose and location of people). Given the prevalence of gender artifacts, 
  we claim that attempts to remove gender artifacts from such datasets are largely infeasible. 
  Instead, the responsibility lies with researchers and practitioners to be aware that the distribution of images within datasets is 
  highly gendered and hence develop methods which are robust to these distributional shifts across groups.

</div>

<!-- Citation -->
<div class="container">
  <h2>Citation</h2>
  <pre><code>
    @misc{https://doi.org/10.48550/arxiv.2206.09191,
    doi = {10.48550/ARXIV.2206.09191},
    url = {https://arxiv.org/abs/2206.09191},
    author = {Meister, Nicole and Zhao, Dora and Wang, Angelina and Ramaswamy, Vikram V. and Fong, Ruth and Russakovsky, Olga},
    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Gender Artifacts in Visual Datasets},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International}
  }
  </code></pre>
</div>

<!-- Talk -->
<!--
<div class="container" >
  <h2>1 Minute Summary</h2>
  <div>
    <div style="width: 75%;height: 0;padding-bottom: 42%;position: relative;margin-left: auto;margin-right: auto;">
      <iframe src="https://youtube.com/embed/7qUzfcn6TPk" allowfullscreen style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"></iframe>
    </div>
  </div>
</div>
-->


<!-- Method -->
<div class="container">
<h2>Demographic annotations on COCO</h2>

We crowdsource skin color and gender annotations on the COCO 2014 validation set
using Amazon Mechanical Turk (AMT). We use the Fitzpatrick Skin Type Scale, which ranges from
1 (lightest) to 6 (darkest) to measure skin color, and the binary perceived gender expression.
In total, we collect annotations for 15,762 images and 28,315 <tt>person</tt> instances.

<br />
<br />
<center>
  <img src="imgs/pie_charts_hoz.png" style="width:80%">
<br />
<br />
</center>
<div class="caption">
  (Left column): Distribution of perceived skin color and gender expression of the
  28,315 <tt>people</tt> instances.
  <br />
  (Middle column): Distribution after collapsing individual annotations into image-level annotations
  <br />
  (Right column): Distribution of self-reported demographics for AMT workers.
</div>
<br />
<br />
The annotations will be available for download upon request.

</div>

<!-- Analysis -->
<div class="container">
<h2>Analysis</h2>
Using the crowdsourced demographic annotations, we consider both the ground-truth images
and manual captions as well as the automatically generated captions.
<br/>
<br/>
<br/>
<center>
  <img src="imgs/findings.png" style="width:75%">
</center>

Our analysis shows that not only does bias exist in the ground-truth data, beyond just
the underepresentation of certain skin tone groups, but also that this bias is propagating
into the generated captions. Furthermore, we find that newer and more advanced image
captioning models tend to exhibit more bias.

<br/>
<br/>
We provide an in-depth analysis of these bias propagation pathways in our paper.


</div>

<!-- Related Work -->
<div class="container" >
  <!-- <h2>Related Work</h2>
  <div>
  Below are some papers related to our work. We discuss them in more detail in the related work section of our paper.
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/1906.06439" target="_blank">Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias.</a>
    Emily Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, Andrew Zaldivar.
    CVPR 2019 Workshop on Fairness Accountability Transparency and Ethics in Computer Vision.
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/1805.09910" target="_blank"> Fairness GAN.</a>
    Prasanna Sattigeri, Samuel C. Hoffman, Vijil Chenthamarakshan, Kush R. Varshney.
    IBM Journal of Research and Development 2019.
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/2004.06524" target="_blank">Contrastive Examples for Addressing the Tyranny of the Majority.</a>
    Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor Darrell, Novi Quadrianto. arXiv 2020
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/1910.12008" target="_blank">Fair Generative Modeling via Weak Supervision.</a>
    Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, Stefano Ermon. ICML 2020
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/2007.06570" target="_blank">Towards Causal Benchmarking of Bias in Face Analysis Algorithms.</a>
    Guha Balakrishnan, Yuanjun Xiong, Wei Xia, Pietro Perona. ECCV 2020
  </div>

  &nbsp;

  <div>
    <a href="https://arxiv.org/abs/1911.11834" target="_blank">Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation.</a>
    Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, Olga Russakovsky. CVPR 2020
  </div> -->
</div>

<!-- Acknowledgements -->
<div class="container">
  <h2>Acknowledgements</h2>

  This material is based upon work supported by the National Science Foundation under Grant No. 1763642, Grant No. 2112562, 
  and the Graduate Research Fellowship to A.W., as well as by the Princeton Engineering Project X Fund. 
  Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) 
  and do not necessarily reflect the views of the National Science Foundation.
  <br/><br/>

  The webpage template was adapted from this
  <a href="https://princetonvisualai.github.io/gan-debiasing/" target="_blank">project page</a>.

</div>

<div class="container" >
  <h2>Contact</h2>
  <div><a href="https://nicolemeister.github.io/" style="color:#1075bc" target="new">Nicole Meister</a> (nmeister@alumni.princeton.edu)</div>
  <div><a href="https://dorazhao99.github.io/" style="color:#1075bc" target="new">Dora Zhao</a> (dorothyz@alumni.princeton.edu)</div>
</div>

<div id="footer">
</div>


</body>
</html>
